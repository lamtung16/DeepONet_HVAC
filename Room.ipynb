{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jzH_LbEIGLWu"
      },
      "outputs": [],
      "source": [
        "# library\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from torchmetrics import R2Score, MeanSquaredError\n",
        "\n",
        "r2score = R2Score()\n",
        "msescore = MeanSquaredError()\n",
        "\n",
        "torch.manual_seed(2)\n",
        "np.random.seed(2)\n",
        "torch.set_printoptions(precision=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sigma function for conditioning (u1 --> y1, u2 --> y2, ...)\n",
        "def sigma(t, k, n):                             # t is the input, k is the size of one control, n is the window length\n",
        "    a = np.array([])\n",
        "    for i in range(n):\n",
        "        for j in range(k):\n",
        "            a = np.append(a, i+1)\n",
        "    a = torch.tensor(a, dtype=torch.float32)\n",
        "    alpha = 16\n",
        "    return 1 - torch.sigmoid(alpha*(a-t-1.5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Y3eaKBL2GhJI"
      },
      "outputs": [],
      "source": [
        "# Model\n",
        "class Net(torch.nn.Module):\n",
        "  def __init__(self, n=4, p=8, noi=1, b0_size = 8, bi_size = 2, trunk_size = 8):\n",
        "    super(Net, self).__init__()\n",
        "    self.n   = n                    # horizon window length\n",
        "    self.p   = p                    # size of branch and trunk output\n",
        "    self.noi = noi                  # number of input\n",
        "    self.k   = int(self.p/self.n)   # size of each sub-branch\n",
        "\n",
        "    self.b0_size    = b0_size\n",
        "    self.bi_size    = bi_size\n",
        "    self.trunk_size = trunk_size\n",
        "    \n",
        "    # Branch x0\n",
        "    self.input_x0  = torch.nn.Linear(1, self.b0_size)\n",
        "    self.hidden_x0 = torch.nn.Linear(self.b0_size, self.b0_size)\n",
        "    self.output_x0 = torch.nn.Linear(self.b0_size, self.p)\n",
        "\n",
        "    # Branch 1 u\n",
        "    self.input1_u  = torch.nn.Linear(self.noi, self.bi_size)\n",
        "    self.hidden1_u = torch.nn.Linear(self.bi_size, self.bi_size)\n",
        "    self.output1_u = torch.nn.Linear(self.bi_size, self.k)\n",
        "\n",
        "    # Branch 2 u\n",
        "    self.input2_u  = torch.nn.Linear(self.noi, self.bi_size)\n",
        "    self.hidden2_u = torch.nn.Linear(self.bi_size, self.bi_size)\n",
        "    self.output2_u = torch.nn.Linear(self.bi_size, self.k)\n",
        "\n",
        "    # Branch 3 u\n",
        "    self.input3_u  = torch.nn.Linear(self.noi, self.bi_size)\n",
        "    self.hidden3_u = torch.nn.Linear(self.bi_size, self.bi_size)\n",
        "    self.output3_u = torch.nn.Linear(self.bi_size, self.k)\n",
        "\n",
        "    # Branch 4 u\n",
        "    self.input4_u  = torch.nn.Linear(self.noi, self.bi_size)\n",
        "    self.hidden4_u = torch.nn.Linear(self.bi_size, self.bi_size)\n",
        "    self.output4_u = torch.nn.Linear(self.bi_size, self.k)\n",
        "\n",
        "    # Trunk\n",
        "    self.input_t   = torch.nn.Linear(1, self.trunk_size)\n",
        "    self.hidden_t  = torch.nn.Linear(self.trunk_size, self.trunk_size)\n",
        "    self.output_t  = torch.nn.Linear(self.trunk_size, self.p)\n",
        "\n",
        "  def forward(self, x0, u, t):\n",
        "    # h\n",
        "    h = torch.selu(self.input_x0(x0))\n",
        "    h = torch.selu(self.hidden_x0(h))\n",
        "    h = self.output_x0(h)\n",
        "\n",
        "    # f\n",
        "    f1 = torch.selu(self.input1_u(u[:,0*self.noi:1*self.noi].reshape(-1,self.noi)))\n",
        "    f1 = torch.selu(self.hidden1_u(f1))\n",
        "    f1 = self.output1_u(f1)\n",
        "\n",
        "    f2 = torch.selu(self.input2_u(u[:,1*self.noi:2*self.noi].reshape(-1,self.noi)))\n",
        "    f2 = torch.selu(self.hidden2_u(f2))\n",
        "    f2 = self.output2_u(f2)\n",
        "\n",
        "    f3 = torch.selu(self.input3_u(u[:,2*self.noi:3*self.noi].reshape(-1,self.noi)))\n",
        "    f3 = torch.selu(self.hidden3_u(f3))\n",
        "    f3 = self.output3_u(f3)\n",
        "\n",
        "    f4 = torch.selu(self.input4_u(u[:,3*self.noi:4*self.noi].reshape(-1,self.noi)))\n",
        "    f4 = torch.selu(self.hidden4_u(f4))\n",
        "    f4 = self.output4_u(f4)\n",
        "\n",
        "    f = torch.cat((f1, f2, f3, f4), dim=1)\n",
        "\n",
        "    # sigma\n",
        "    s = sigma(t, self.k, self.n)\n",
        "\n",
        "    # g\n",
        "    g = torch.selu(self.input_t(t))\n",
        "    g = torch.selu(self.hidden_t(g))\n",
        "    g = self.output_t(g)\n",
        "\n",
        "    return torch.sum(h*f*s*g + x0, dim=1).reshape(-1,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BtNtb3epPIzJ"
      },
      "outputs": [],
      "source": [
        "# Model error\n",
        "def eval(model, testset):\n",
        "    with torch.no_grad():\n",
        "        pred_Y = model(testset.x0_data, testset.u_data, testset.t_data)\n",
        "\n",
        "    r2  = r2score(pred_Y, testset.y_data)\n",
        "    mse = msescore(pred_Y, testset.y_data)\n",
        "    return r2.item(), mse.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dTWtNEdVHCuw"
      },
      "outputs": [],
      "source": [
        "# Data\n",
        "class Data(torch.utils.data.Dataset):\n",
        "  def __init__(self, src_file, n, H, noi):\n",
        "    self.n   = n                                  # horizon length\n",
        "    self.H   = H                                  # max window length\n",
        "    self.noi = noi                                # number of input\n",
        "    self.src_file = src_file                      # source file\n",
        "    df = pd.read_csv(self.src_file, header=None)\n",
        "\n",
        "    X0, U, T, Y = np.array([[1]], dtype=np.float32), np.ones((1, n*self.noi)), np.array([[1]], dtype=np.float32), np.array([[1]], dtype=np.float32)\n",
        "    for i in range(df.shape[0]):\n",
        "        row = np.array(df.iloc[i])\n",
        "        for j in range(self.H - self.n):\n",
        "            x0 = np.array([[row[self.H*self.noi + j]]])\n",
        "            u  = np.array([row[j:j + self.n*self.noi]])\n",
        "            for t in range(1, self.n + 1):\n",
        "                y = np.array([[row[self.H*self.noi + j + t]]])\n",
        "                t = np.array([[t]])\n",
        "\n",
        "                X0 = np.concatenate((X0, x0))\n",
        "                U  = np.concatenate((U, u))\n",
        "                T  = np.concatenate((T, t))\n",
        "                Y  = np.concatenate((Y, y))\n",
        "\n",
        "    X0, U, T, Y = X0[1:], U[1:], T[1:], Y[1:]\n",
        "\n",
        "    self.x0_data = torch.tensor(X0, dtype=torch.float32)\n",
        "    self.u_data  = torch.tensor(U,  dtype=torch.float32)\n",
        "    self.t_data  = torch.tensor(T,  dtype=torch.float32)\n",
        "    self.y_data  = torch.tensor(Y,  dtype=torch.float32)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.x0_data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    if torch.is_tensor(idx):\n",
        "      idx = idx.tolist()\n",
        "    x0  = self.x0_data[idx]\n",
        "    u   = self.u_data[idx]\n",
        "    t   = self.t_data[idx]\n",
        "    y   = self.y_data[idx]\n",
        "    sample = {'x0':x0, 'u':u, 't':t, 'y':y}\n",
        "    return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Early stopping\n",
        "def early_stop(list, min_epochs, patience):\n",
        "    if(len(list) > min_epochs):\n",
        "        if(np.max(list[-patience:]) < 1.0001*np.max(list[0: -patience])):\n",
        "            return 1\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot\n",
        "def plot(net, dataset, size):\n",
        "    with torch.no_grad():\n",
        "        pred_Y = net(dataset.x0_data, dataset.u_data, dataset.t_data)\n",
        "\n",
        "    plt.figure(figsize=size)\n",
        "    plt.plot(dataset.y_data[0::4], 'b',   label=r'real',      linewidth=3)\n",
        "    plt.plot(pred_Y[0::4],         'r--', label=r'predicted', linewidth=1)\n",
        "    plt.ylabel(r'x(t)')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train function\n",
        "def train(net, train_ds, test_ds, lr=0.001, min_epochs=200, max_epochs=100000, patience=100):\n",
        "    loss_func  = torch.nn.MSELoss()\n",
        "    optimizer  = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "    train_ldr = torch.utils.data.DataLoader(train_ds, batch_size=train_ds.y_data.shape[0], shuffle=True)\n",
        "\n",
        "    R2  = np.array([])\n",
        "    MSE = np.array([])\n",
        "    for epoch in range(0, max_epochs+1):\n",
        "        net.train()\n",
        "        loss  = 0\n",
        "        count = 0\n",
        "        for (_, batch) in enumerate(train_ldr):\n",
        "            X0 = batch['x0']\n",
        "            U  = batch['u']\n",
        "            T  = batch['t']\n",
        "            Y  = batch['y']\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = net(X0, U, T)             # compute the output of the Network\n",
        "            loss_val = loss_func(output, Y)    # loss function\n",
        "            loss += loss_val.item()            # accumulate\n",
        "            loss_val.backward()                # gradients\n",
        "            optimizer.step()                   # update paramters\n",
        "            count += 1\n",
        "        \n",
        "        net.eval()\n",
        "        R2  = np.append(R2, eval(net, test_ds)[0])\n",
        "        MSE = np.append(MSE, eval(net, test_ds)[1])\n",
        "\n",
        "        # if(epoch%500==0):\n",
        "        #     print(\"epoch = %5d \\t loss = %12.4f \\t R2 = %12.4f \\t MSE = %12.4f\" % (epoch, loss/count, eval(net, test_ds)[0], eval(net, test_ds)[1]))\n",
        "        \n",
        "        if(early_stop(list = R2, min_epochs = min_epochs, patience = patience) == 1):\n",
        "            break\n",
        "    \n",
        "    return R2, MSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gebr4CnRLFBd",
        "outputId": "d74e7cd7-8dab-49a9-babc-e77577dd73b7"
      },
      "outputs": [],
      "source": [
        "df_result = pd.DataFrame({'lr':[], 'p':[], 'b0':[], 'bi':[], 'trunk':[], 'best_epoch':[], 'R2':[], 'MSE':[]})\n",
        "\n",
        "for _lr in [0.0001, 0.001, 0.01]:\n",
        "    for _p in [4, 8, 12]:\n",
        "        for b0 in [4, 8, 12]:\n",
        "            for bi in [2, 4, 8]:\n",
        "                for trunk in [4, 8, 12]:\n",
        "                    # Hyperparameters\n",
        "                    p   = _p          # size of branch and trunk ouput\n",
        "                    n   = 4           # horizon window length\n",
        "                    noi = 2           # number of inputs\n",
        "                    H   = 512         # maximum window length\n",
        "\n",
        "                    # Create Dataset and DataLoader objects\n",
        "                    src_file_train = '0. Data/data_0.csv'\n",
        "                    train_ds       = Data(src_file_train, n, H, noi)\n",
        "\n",
        "                    src_file_test  = '0. Data/data_1.csv'\n",
        "                    test_ds        = Data(src_file_test, n, H, noi)\n",
        "\n",
        "                    # Create network\n",
        "                    device = torch.device(\"cpu\")\n",
        "                    net = Net(n, p, noi, b0_size=b0, bi_size=bi, trunk_size=trunk).to(device)\n",
        "\n",
        "                    # train model\n",
        "                    lr         = _lr\n",
        "                    min_epochs = 300\n",
        "                    max_epochs = 100000\n",
        "                    patience   = 200\n",
        "                    R2, MSE = train(net, train_ds, test_ds, lr, min_epochs, max_epochs, patience)\n",
        "\n",
        "                    # plot\n",
        "                    # plot(net, train_ds, (20,2))\n",
        "                    # plot(net, test_ds,  (20,2))\n",
        "\n",
        "                    df_result.loc[len(df_result)] = [_lr, _p, b0, bi, trunk, np.argmax(R2), np.max(R2), np.min(MSE)]\n",
        "\n",
        "                    # save\n",
        "                    PATH = '2. Saved model/DeepONet_HVAC' + str(_lr) + str(_p) + str(b0) + str(bi) + str(trunk) + '.pt'\n",
        "                    torch.save(net, PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_result.to_csv(\"stat.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         lr     p    b0   bi  trunk  best_epoch          R2         MSE\n",
            "0    0.0001   4.0   4.0  2.0    4.0     30404.0      0.9837      0.0519\n",
            "1    0.0001   4.0   4.0  2.0    8.0     11142.0      0.9286      0.2273\n",
            "2    0.0001   4.0   4.0  2.0   12.0     14454.0      0.5917      1.2998\n",
            "3    0.0001   4.0   4.0  4.0    4.0     25145.0      0.9822      0.0568\n",
            "4    0.0001   4.0   4.0  4.0    8.0     16808.0      0.6139      1.2292\n",
            "5    0.0001   4.0   4.0  4.0   12.0     10501.0    -33.5438    109.9724\n",
            "6    0.0001   4.0   4.0  8.0    4.0     10423.0      0.1527      2.6974\n",
            "7    0.0001   4.0   4.0  8.0    8.0     22548.0      0.9807      0.0614\n",
            "8    0.0001   4.0   4.0  8.0   12.0     17493.0      0.8419      0.5034\n",
            "9    0.0001   4.0   8.0  2.0    4.0     12700.0      0.9807      0.0614\n",
            "10   0.0001   4.0   8.0  2.0    8.0     15300.0      0.9717      0.0903\n",
            "11   0.0001   4.0   8.0  2.0   12.0     19489.0      0.9032      0.3081\n",
            "12   0.0001   4.0   8.0  4.0    4.0     37129.0      0.9812      0.0599\n",
            "13   0.0001   4.0   8.0  4.0    8.0      9888.0    -51.4742    167.0548\n",
            "14   0.0001   4.0   8.0  4.0   12.0     22363.0      0.9859      0.0448\n",
            "15   0.0001   4.0   8.0  8.0    4.0     17842.0      0.9312      0.2190\n",
            "16   0.0001   4.0   8.0  8.0    8.0     25309.0      0.9779      0.0702\n",
            "17   0.0001   4.0   8.0  8.0   12.0     14754.0     -0.3127      4.1790\n",
            "18   0.0001   4.0  12.0  2.0    4.0      9631.0      0.9392      0.1935\n",
            "19   0.0001   4.0  12.0  2.0    8.0     23019.0      0.9876      0.0394\n",
            "20   0.0001   4.0  12.0  2.0   12.0      8575.0      0.9489      0.1627\n",
            "21   0.0001   4.0  12.0  4.0    4.0       264.0  -1165.8604   3714.7737\n",
            "22   0.0001   4.0  12.0  4.0    8.0     16785.0      0.8483      0.4831\n",
            "23   0.0001   4.0  12.0  4.0   12.0     16588.0      0.9856      0.0459\n",
            "24   0.0001   4.0  12.0  8.0    4.0     22122.0      0.9386      0.1954\n",
            "25   0.0001   4.0  12.0  8.0    8.0     34925.0      0.9761      0.0761\n",
            "26   0.0001   4.0  12.0  8.0   12.0     15453.0      0.7796      0.7017\n",
            "27   0.0001   8.0   4.0  2.0    4.0     22353.0      0.8008      0.6341\n",
            "28   0.0001   8.0   4.0  2.0    8.0      6051.0  -1998.7377   6366.2910\n",
            "29   0.0001   8.0   4.0  2.0   12.0     12612.0      0.0022      3.1764\n",
            "30   0.0001   8.0   4.0  4.0    4.0     17950.0      0.6127      1.2331\n",
            "31   0.0001   8.0   4.0  4.0    8.0     15058.0     -1.7503      8.7558\n",
            "32   0.0001   8.0   4.0  4.0   12.0     43184.0      0.8833      0.3716\n",
            "33   0.0001   8.0   4.0  8.0    4.0     14463.0     -1.2458      7.1498\n",
            "34   0.0001   8.0   4.0  8.0    8.0     23185.0     -0.2724      4.0507\n",
            "35   0.0001   8.0   4.0  8.0   12.0     20030.0     -0.7280      5.5012\n",
            "36   0.0001   8.0   8.0  2.0    4.0     34018.0      0.9824      0.0560\n",
            "37   0.0001   8.0   8.0  2.0    8.0     24386.0      0.3905      1.9405\n",
            "38   0.0001   8.0   8.0  2.0   12.0     19387.0      0.8792      0.3847\n",
            "39   0.0001   8.0   8.0  4.0    4.0     15596.0      0.6415      1.1413\n",
            "40   0.0001   8.0   8.0  4.0    8.0     55241.0      0.9672      0.1044\n",
            "41   0.0001   8.0   8.0  4.0   12.0       217.0  -7000.5864  22289.9922\n",
            "42   0.0001   8.0   8.0  8.0    4.0     14964.0     -0.0391      3.3080\n",
            "43   0.0001   8.0   8.0  8.0    8.0     14408.0      0.2551      2.3715\n",
            "44   0.0001   8.0   8.0  8.0   12.0     23748.0      0.5637      1.3891\n",
            "45   0.0001   8.0  12.0  2.0    4.0     43326.0      0.9579      0.1341\n",
            "46   0.0001   8.0  12.0  2.0    8.0     22375.0      0.9670      0.1050\n",
            "47   0.0001   8.0  12.0  2.0   12.0     26099.0      0.9857      0.0454\n",
            "48   0.0001   8.0  12.0  4.0    4.0     28702.0      0.8851      0.3657\n",
            "49   0.0001   8.0  12.0  4.0    8.0     15419.0      0.6440      1.1334\n",
            "50   0.0001   8.0  12.0  4.0   12.0     21434.0      0.9233      0.2443\n",
            "51   0.0001   8.0  12.0  8.0    4.0     21977.0     -4.1276     16.3240\n",
            "52   0.0001   8.0  12.0  8.0    8.0     43782.0      0.8192      0.5756\n",
            "53   0.0001   8.0  12.0  8.0   12.0     48529.0      0.0493      3.0266\n",
            "54   0.0001  12.0   4.0  2.0    4.0      8648.0   -137.8526    442.0460\n",
            "55   0.0001  12.0   4.0  2.0    8.0     18886.0      0.7311      0.8561\n",
            "56   0.0001  12.0   4.0  2.0   12.0     12532.0   -212.3946    679.3552\n",
            "57   0.0001  12.0   4.0  4.0    4.0     29596.0      0.9246      0.2400\n",
            "58   0.0001  12.0   4.0  4.0    8.0     20588.0     -0.6780      5.3419\n",
            "59   0.0001  12.0   4.0  4.0   12.0     28073.0      0.8295      0.5428\n",
            "60   0.0001  12.0   4.0  8.0    4.0     27483.0      0.6131      1.2317\n",
            "61   0.0001  12.0   4.0  8.0    8.0     20085.0      0.3419      2.0952\n",
            "62   0.0001  12.0   4.0  8.0   12.0     18181.0     -0.0289      3.2756\n",
            "63   0.0001  12.0   8.0  2.0    4.0       768.0 -19774.4160  62956.2852\n",
            "64   0.0001  12.0   8.0  2.0    8.0     28730.0      0.9579      0.1340\n",
            "65   0.0001  12.0   8.0  2.0   12.0      6906.0    -24.7647     82.0234\n",
            "66   0.0001  12.0   8.0  4.0    4.0     24067.0     -4.6165     17.8804\n",
            "67   0.0001  12.0   8.0  4.0    8.0     47731.0      0.3867      1.9524\n",
            "68   0.0001  12.0   8.0  4.0   12.0     18369.0      0.6447      1.1310\n",
            "69   0.0001  12.0   8.0  8.0    4.0     11988.0    -15.1253     51.3358\n",
            "70   0.0001  12.0   8.0  8.0    8.0     19059.0     -3.9657     15.8085\n",
            "71   0.0001  12.0   8.0  8.0   12.0     33319.0      0.3255      2.1473\n",
            "72   0.0001  12.0  12.0  2.0    4.0     17429.0      0.4373      1.7915\n",
            "73   0.0001  12.0  12.0  2.0    8.0     13746.0      0.9111      0.2830\n",
            "74   0.0001  12.0  12.0  2.0   12.0     37117.0     -0.4174      4.5123\n",
            "75   0.0001  12.0  12.0  4.0    4.0     13701.0     -1.6869      8.5539\n",
            "76   0.0001  12.0  12.0  4.0    8.0     15591.0     -4.6671     18.0416\n",
            "77   0.0001  12.0  12.0  4.0   12.0     61162.0      0.6234      1.1989\n",
            "78   0.0001  12.0  12.0  8.0    4.0     18337.0     -3.7456     15.1078\n",
            "79   0.0001  12.0  12.0  8.0    8.0     12119.0     -2.5978     11.4539\n",
            "80   0.0001  12.0  12.0  8.0   12.0     44769.0      0.7207      0.8892\n",
            "81   0.0010   4.0   4.0  2.0    4.0     13657.0      0.9852      0.0472\n",
            "82   0.0010   4.0   4.0  2.0    8.0      4396.0      0.8910      0.3472\n",
            "83   0.0010   4.0   4.0  2.0   12.0      4496.0      0.9734      0.0847\n",
            "84   0.0010   4.0   4.0  4.0    4.0     10011.0      0.9034      0.3074\n",
            "85   0.0010   4.0   4.0  4.0    8.0     11939.0      0.9871      0.0412\n",
            "86   0.0010   4.0   4.0  4.0   12.0      5519.0      0.5610      1.3976\n",
            "87   0.0010   4.0   4.0  8.0    4.0      2894.0    -13.7660     47.0086\n",
            "88   0.0010   4.0   4.0  8.0    8.0      4253.0     -0.7032      5.4224\n",
            "89   0.0010   4.0   4.0  8.0   12.0      7997.0      0.5511      1.4292\n",
            "90   0.0010   4.0   8.0  2.0    4.0      7671.0      0.9859      0.0449\n",
            "91   0.0010   4.0   8.0  2.0    8.0      6451.0      0.9249      0.2391\n",
            "92   0.0010   4.0   8.0  2.0   12.0      2040.0      0.7798      0.7011\n",
            "93   0.0010   4.0   8.0  4.0    4.0     11227.0      0.5262      1.5085\n",
            "94   0.0010   4.0   8.0  4.0    8.0     10670.0      0.9314      0.2185\n",
            "95   0.0010   4.0   8.0  4.0   12.0      3786.0      0.7310      0.8563\n",
            "96   0.0010   4.0   8.0  8.0    4.0      4730.0      0.5475      1.4406\n",
            "97   0.0010   4.0   8.0  8.0    8.0     12530.0      0.9161      0.2670\n",
            "98   0.0010   4.0   8.0  8.0   12.0      2114.0     -7.5058     27.0786\n",
            "99   0.0010   4.0  12.0  2.0    4.0      8200.0      0.9768      0.0739\n",
            "100  0.0010   4.0  12.0  2.0    8.0      8519.0      0.9867      0.0422\n",
            "101  0.0010   4.0  12.0  2.0   12.0      1823.0      0.9177      0.2620\n",
            "102  0.0010   4.0  12.0  4.0    4.0      3491.0      0.6670      1.0602\n",
            "103  0.0010   4.0  12.0  4.0    8.0      9004.0      0.9844      0.0495\n",
            "104  0.0010   4.0  12.0  4.0   12.0      8676.0      0.5429      1.4552\n",
            "105  0.0010   4.0  12.0  8.0    4.0      9453.0      0.5806      1.3353\n",
            "106  0.0010   4.0  12.0  8.0    8.0     10636.0      0.6999      0.9553\n",
            "107  0.0010   4.0  12.0  8.0   12.0      7567.0      0.7847      0.6853\n",
            "108  0.0010   8.0   4.0  2.0    4.0        26.0  -7040.5884  22417.3398\n",
            "109  0.0010   8.0   4.0  2.0    8.0      8261.0      0.4567      1.7297\n",
            "110  0.0010   8.0   4.0  2.0   12.0      8632.0      0.8012      0.6329\n",
            "111  0.0010   8.0   4.0  4.0    4.0      4188.0    -83.9440    270.4247\n",
            "112  0.0010   8.0   4.0  4.0    8.0      7194.0     -0.0132      3.2256\n",
            "113  0.0010   8.0   4.0  4.0   12.0     12817.0      0.8548      0.4623\n",
            "114  0.0010   8.0   4.0  8.0    4.0      7336.0     -1.9001      9.2328\n",
            "115  0.0010   8.0   4.0  8.0    8.0      8607.0     -1.8022      8.9208\n",
            "116  0.0010   8.0   4.0  8.0   12.0      8406.0     -0.5567      4.9558\n",
            "117  0.0010   8.0   8.0  2.0    4.0     24097.0      0.9876      0.0396\n",
            "118  0.0010   8.0   8.0  2.0    8.0      6912.0      0.9851      0.0474\n",
            "119  0.0010   8.0   8.0  2.0   12.0      8357.0      0.9855      0.0460\n",
            "120  0.0010   8.0   8.0  4.0    4.0      4580.0     -2.7499     11.9380\n",
            "121  0.0010   8.0   8.0  4.0    8.0      8596.0      0.7456      0.8099\n",
            "122  0.0010   8.0   8.0  4.0   12.0      7023.0     -2.5183     11.2006\n",
            "123  0.0010   8.0   8.0  8.0    4.0      8949.0      0.3060      2.2094\n",
            "124  0.0010   8.0   8.0  8.0    8.0      7941.0      0.5696      1.3701\n",
            "125  0.0010   8.0   8.0  8.0   12.0        52.0  -6766.9722  21546.2656\n",
            "126  0.0010   8.0  12.0  2.0    4.0      3792.0      0.7922      0.6617\n",
            "127  0.0010   8.0  12.0  2.0    8.0      4689.0     -2.2459     10.3335\n",
            "128  0.0010   8.0  12.0  2.0   12.0      7245.0      0.9783      0.0692\n",
            "129  0.0010   8.0  12.0  4.0    4.0      9064.0     -0.0419      3.3169\n",
            "130  0.0010   8.0  12.0  4.0    8.0      7510.0      0.8078      0.6119\n",
            "131  0.0010   8.0  12.0  4.0   12.0      6582.0      0.5713      1.3648\n",
            "132  0.0010   8.0  12.0  8.0    4.0      3238.0    -26.0542     86.1288\n",
            "133  0.0010   8.0  12.0  8.0    8.0      3905.0     -2.3551     10.6812\n",
            "134  0.0010   8.0  12.0  8.0   12.0     21599.0      0.6147      1.2268\n",
            "135  0.0010  12.0   4.0  2.0    4.0      3412.0    -13.8021     47.1236\n",
            "136  0.0010  12.0   4.0  2.0    8.0      5480.0      0.8052      0.6200\n",
            "137  0.0010  12.0   4.0  2.0   12.0      5847.0     -1.2705      7.2282\n",
            "138  0.0010  12.0   4.0  4.0    4.0      3164.0   -214.7195    686.7568\n",
            "139  0.0010  12.0   4.0  4.0    8.0     13561.0      0.4121      1.8718\n",
            "140  0.0010  12.0   4.0  4.0   12.0      5521.0     -3.7727     15.1941\n",
            "141  0.0010  12.0   4.0  8.0    4.0      6054.0     -2.2869     10.4640\n",
            "142  0.0010  12.0   4.0  8.0    8.0      7033.0     -5.6293     21.1048\n",
            "143  0.0010  12.0   4.0  8.0   12.0      3825.0   -170.2204    545.0909\n",
            "144  0.0010  12.0   8.0  2.0    4.0      7123.0      0.6077      1.2488\n",
            "145  0.0010  12.0   8.0  2.0    8.0      7078.0     -0.2440      3.9604\n",
            "146  0.0010  12.0   8.0  2.0   12.0      9933.0     -0.3426      4.2743\n",
            "147  0.0010  12.0   8.0  4.0    4.0      6294.0     -2.1464     10.0168\n",
            "148  0.0010  12.0   8.0  4.0    8.0      6382.0     -2.0289      9.6428\n",
            "149  0.0010  12.0   8.0  4.0   12.0      7399.0      0.2840      2.2794\n",
            "150  0.0010  12.0   8.0  8.0    4.0        19.0 -14613.7744  46527.0547\n",
            "151  0.0010  12.0   8.0  8.0    8.0      2068.0    -10.9839     38.1515\n",
            "152  0.0010  12.0   8.0  8.0   12.0      8461.0     -0.2778      4.0680\n",
            "153  0.0010  12.0  12.0  2.0    4.0     13917.0      0.9770      0.0733\n",
            "154  0.0010  12.0  12.0  2.0    8.0      8042.0      0.8041      0.6238\n",
            "155  0.0010  12.0  12.0  2.0   12.0      2781.0     -9.4140     33.1536\n",
            "156  0.0010  12.0  12.0  4.0    4.0      9565.0      0.4685      1.6921\n",
            "157  0.0010  12.0  12.0  4.0    8.0     11259.0     -0.3381      4.2599\n",
            "158  0.0010  12.0  12.0  4.0   12.0      1804.0   -210.5080    673.3492\n",
            "159  0.0010  12.0  12.0  8.0    4.0      7391.0     -5.8112     21.6839\n",
            "160  0.0010  12.0  12.0  8.0    8.0     10042.0     -0.5903      5.0628\n",
            "161  0.0010  12.0  12.0  8.0   12.0      8399.0     -0.9297      6.1433\n",
            "162  0.0100   4.0   4.0  2.0    4.0      1165.0    -12.5578     43.1620\n",
            "163  0.0100   4.0   4.0  2.0    8.0      5915.0      0.9868      0.0421\n",
            "164  0.0100   4.0   4.0  2.0   12.0      2832.0      0.7563      0.7758\n",
            "165  0.0100   4.0   4.0  4.0    4.0      2664.0      0.2431      2.4096\n",
            "166  0.0100   4.0   4.0  4.0    8.0      2501.0     -2.7228     11.8518\n",
            "167  0.0100   4.0   4.0  4.0   12.0      4168.0      0.8373      0.5181\n",
            "168  0.0100   4.0   4.0  8.0    4.0      3543.0      0.7652      0.7476\n",
            "169  0.0100   4.0   4.0  8.0    8.0       657.0    -27.4167     90.4663\n",
            "170  0.0100   4.0   4.0  8.0   12.0      1723.0      0.6877      0.9941\n",
            "171  0.0100   4.0   8.0  2.0    4.0      3113.0      0.9107      0.2842\n",
            "172  0.0100   4.0   8.0  2.0    8.0       363.0      0.7582      0.7698\n",
            "173  0.0100   4.0   8.0  2.0   12.0      1982.0      0.7826      0.6923\n",
            "174  0.0100   4.0   8.0  4.0    4.0       989.0     -1.8128      8.9548\n",
            "175  0.0100   4.0   8.0  4.0    8.0      4275.0      0.6842      1.0053\n",
            "176  0.0100   4.0   8.0  4.0   12.0      3187.0      0.0532      3.0142\n",
            "177  0.0100   4.0   8.0  8.0    4.0      3855.0     -0.4550      4.6322\n",
            "178  0.0100   4.0   8.0  8.0    8.0      1642.0     -0.6464      5.2413\n",
            "179  0.0100   4.0   8.0  8.0   12.0      1775.0      0.3285      2.1379\n",
            "180  0.0100   4.0  12.0  2.0    4.0      2102.0      0.8228      0.5640\n",
            "181  0.0100   4.0  12.0  2.0    8.0      2886.0      0.9816      0.0587\n",
            "182  0.0100   4.0  12.0  2.0   12.0      2170.0      0.8168      0.5833\n",
            "183  0.0100   4.0  12.0  4.0    4.0      5574.0      0.9687      0.0998\n",
            "184  0.0100   4.0  12.0  4.0    8.0       619.0    -33.3081    109.2221\n",
            "185  0.0100   4.0  12.0  4.0   12.0      2982.0      0.6890      0.9901\n",
            "186  0.0100   4.0  12.0  8.0    4.0      1746.0     -5.5825     20.9559\n",
            "187  0.0100   4.0  12.0  8.0    8.0      4207.0      0.6631      1.0725\n",
            "188  0.0100   4.0  12.0  8.0   12.0      2536.0      0.1054      2.8480\n",
            "189  0.0100   8.0   4.0  2.0    4.0      1120.0      0.0360      3.0690\n",
            "190  0.0100   8.0   4.0  2.0    8.0      1899.0      0.5846      1.3225\n",
            "191  0.0100   8.0   4.0  2.0   12.0        83.0    -14.4861     49.3008\n",
            "192  0.0100   8.0   4.0  4.0    4.0      6528.0      0.8338      0.5292\n",
            "193  0.0100   8.0   4.0  4.0    8.0      2889.0      0.8119      0.5989\n",
            "194  0.0100   8.0   4.0  4.0   12.0       888.0     -3.9078     15.6244\n",
            "195  0.0100   8.0   4.0  8.0    4.0      3888.0      0.0745      2.9463\n",
            "196  0.0100   8.0   4.0  8.0    8.0      2264.0     -5.4113     20.4109\n",
            "197  0.0100   8.0   4.0  8.0   12.0      2011.0     -1.6034      8.2880\n",
            "198  0.0100   8.0   8.0  2.0    4.0      2271.0      0.2883      2.2658\n",
            "199  0.0100   8.0   8.0  2.0    8.0      6123.0      0.9851      0.0475\n",
            "200  0.0100   8.0   8.0  2.0   12.0       587.0     -3.9231     15.6729\n",
            "201  0.0100   8.0   8.0  4.0    4.0      1347.0     -0.2386      3.9431\n",
            "202  0.0100   8.0   8.0  4.0    8.0      4444.0      0.9802      0.0631\n",
            "203  0.0100   8.0   8.0  4.0   12.0      3223.0     -0.3428      4.2750\n",
            "204  0.0100   8.0   8.0  8.0    4.0      2328.0     -1.2856      7.2764\n",
            "205  0.0100   8.0   8.0  8.0    8.0      2707.0     -2.5906     11.4308\n",
            "206  0.0100   8.0   8.0  8.0   12.0      2567.0     -0.7756      5.6527\n",
            "207  0.0100   8.0  12.0  2.0    4.0      1526.0     -1.9290      9.3248\n",
            "208  0.0100   8.0  12.0  2.0    8.0       988.0     -4.2022     16.5616\n",
            "209  0.0100   8.0  12.0  2.0   12.0      1186.0      0.4270      1.8242\n",
            "210  0.0100   8.0  12.0  4.0    4.0      2695.0     -0.6158      5.1440\n",
            "211  0.0100   8.0  12.0  4.0    8.0      1933.0      0.5649      1.3850\n",
            "212  0.0100   8.0  12.0  4.0   12.0      5450.0      0.8289      0.5448\n",
            "213  0.0100   8.0  12.0  8.0    4.0      4114.0      0.2287      2.4554\n",
            "214  0.0100   8.0  12.0  8.0    8.0      3562.0      0.5026      1.5836\n",
            "215  0.0100   8.0  12.0  8.0   12.0      1106.0    -19.4878     65.2242\n",
            "216  0.0100  12.0   4.0  2.0    4.0      2707.0      0.6012      1.2696\n",
            "217  0.0100  12.0   4.0  2.0    8.0      1911.0      0.6493      1.1166\n",
            "218  0.0100  12.0   4.0  2.0   12.0      9250.0      0.9782      0.0695\n",
            "219  0.0100  12.0   4.0  4.0    4.0      4909.0      0.9306      0.2210\n",
            "220  0.0100  12.0   4.0  4.0    8.0      3343.0      0.7151      0.9071\n",
            "221  0.0100  12.0   4.0  4.0   12.0      2162.0    -71.6846    231.3961\n",
            "222  0.0100  12.0   4.0  8.0    4.0      3858.0    -17.3520     58.4246\n",
            "223  0.0100  12.0   4.0  8.0    8.0      3333.0     -5.1714     19.6470\n",
            "224  0.0100  12.0   4.0  8.0   12.0      1334.0    -44.8058    145.8256\n",
            "225  0.0100  12.0   8.0  2.0    4.0       989.0     -5.3966     20.3639\n",
            "226  0.0100  12.0   8.0  2.0    8.0      2951.0      0.8063      0.6165\n",
            "227  0.0100  12.0   8.0  2.0   12.0      2105.0      0.1548      2.6907\n",
            "228  0.0100  12.0   8.0  4.0    4.0      1566.0     -8.8020     31.2054\n",
            "229  0.0100  12.0   8.0  4.0    8.0      4323.0     -4.4604     17.3837\n",
            "230  0.0100  12.0   8.0  4.0   12.0      2758.0     -5.3910     20.3461\n",
            "231  0.0100  12.0   8.0  8.0    4.0      4117.0     -0.6553      5.2697\n",
            "232  0.0100  12.0   8.0  8.0    8.0      1496.0    -26.0681     86.1729\n",
            "233  0.0100  12.0   8.0  8.0   12.0      2417.0     -3.5585     14.5122\n",
            "234  0.0100  12.0  12.0  2.0    4.0      1356.0     -5.4062     20.3946\n",
            "235  0.0100  12.0  12.0  2.0    8.0      1207.0     -1.3457      7.4676\n",
            "236  0.0100  12.0  12.0  2.0   12.0       916.0     -4.9201     18.8469\n",
            "237  0.0100  12.0  12.0  4.0    4.0      1332.0     -4.3395     16.9987\n",
            "238  0.0100  12.0  12.0  4.0    8.0      1777.0     -1.3440      7.4621\n",
            "239  0.0100  12.0  12.0  4.0   12.0      2058.0     -6.3298     23.3349\n",
            "240  0.0100  12.0  12.0  8.0    4.0      1858.0     -4.0301     16.0136\n",
            "241  0.0100  12.0  12.0  8.0    8.0      2697.0     -0.7992      5.7280\n",
            "242  0.0100  12.0  12.0  8.0   12.0      1936.0    -14.1561     48.2503\n"
          ]
        }
      ],
      "source": [
        "with pd.option_context('display.max_rows', None,\n",
        "                       'display.max_columns', None,\n",
        "                       'display.precision', 4,\n",
        "                       ):\n",
        "    print(df_result)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "3. T_z .ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "912d6611990680b3d240e982c9d50f3da4c776707cfd42695cf7d82c88d80956"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

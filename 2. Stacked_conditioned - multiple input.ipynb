{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jzH_LbEIGLWu"
      },
      "outputs": [],
      "source": [
        "# library\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from torchmetrics import R2Score, MeanSquaredError\n",
        "\n",
        "r2score = R2Score()\n",
        "msescore = MeanSquaredError()\n",
        "\n",
        "torch.manual_seed(2)\n",
        "np.random.seed(2)\n",
        "torch.set_printoptions(precision=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sigma function for conditioning (u1 --> y1, u2 --> y2, ...)\n",
        "def sigma(t, k, n):                             # t is the input, k is the size of one control, n is the window length\n",
        "    a = np.array([])\n",
        "    for i in range(n):\n",
        "        for j in range(k):\n",
        "            a = np.append(a, i+1)\n",
        "    a = torch.tensor(a, dtype=torch.float32)\n",
        "    alpha = 16\n",
        "    return 1 - torch.sigmoid(alpha*(a-t-1.5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Y3eaKBL2GhJI"
      },
      "outputs": [],
      "source": [
        "# Model\n",
        "class Net(torch.nn.Module):\n",
        "  def __init__(self, n=4, p=8, noi=1, b0_size = 8, bi_size = 2, trunk_size = 8):\n",
        "    super(Net, self).__init__()\n",
        "    self.n   = n                    # horizon window length\n",
        "    self.p   = p                    # size of branch and trunk output\n",
        "    self.noi = noi                  # number of input\n",
        "    self.k   = int(self.p/self.n)   # size of each sub-branch\n",
        "\n",
        "    self.b0_size    = b0_size\n",
        "    self.bi_size    = bi_size\n",
        "    self.trunk_size = trunk_size\n",
        "    \n",
        "    # Branch x0\n",
        "    self.input_x0  = torch.nn.Linear(1, self.b0_size)\n",
        "    self.hidden_x0 = torch.nn.Linear(self.b0_size, self.b0_size)\n",
        "    self.output_x0 = torch.nn.Linear(self.b0_size, self.p)\n",
        "\n",
        "    # Branch 1 u\n",
        "    self.input1_u  = torch.nn.Linear(self.noi, self.bi_size)\n",
        "    self.hidden1_u = torch.nn.Linear(self.bi_size, self.bi_size)\n",
        "    self.output1_u = torch.nn.Linear(self.bi_size, self.k)\n",
        "\n",
        "    # Branch 2 u\n",
        "    self.input2_u  = torch.nn.Linear(self.noi, self.bi_size)\n",
        "    self.hidden2_u = torch.nn.Linear(self.bi_size, self.bi_size)\n",
        "    self.output2_u = torch.nn.Linear(self.bi_size, self.k)\n",
        "\n",
        "    # Branch 3 u\n",
        "    self.input3_u  = torch.nn.Linear(self.noi, self.bi_size)\n",
        "    self.hidden3_u = torch.nn.Linear(self.bi_size, self.bi_size)\n",
        "    self.output3_u = torch.nn.Linear(self.bi_size, self.k)\n",
        "\n",
        "    # Branch 4 u\n",
        "    self.input4_u  = torch.nn.Linear(self.noi, self.bi_size)\n",
        "    self.hidden4_u = torch.nn.Linear(self.bi_size, self.bi_size)\n",
        "    self.output4_u = torch.nn.Linear(self.bi_size, self.k)\n",
        "\n",
        "    # Trunk\n",
        "    self.input_t   = torch.nn.Linear(1, self.trunk_size)\n",
        "    self.hidden_t  = torch.nn.Linear(self.trunk_size, self.trunk_size)\n",
        "    self.output_t  = torch.nn.Linear(self.trunk_size, self.p)\n",
        "\n",
        "  def forward(self, x0, u, t):\n",
        "    # h\n",
        "    h = torch.selu(self.input_x0(x0))\n",
        "    h = torch.selu(self.hidden_x0(h))\n",
        "    h = self.output_x0(h)\n",
        "\n",
        "    # f\n",
        "    f1 = torch.selu(self.input1_u(u[:,0*self.noi:1*self.noi].reshape(-1,self.noi)))\n",
        "    f1 = torch.selu(self.hidden1_u(f1))\n",
        "    f1 = self.output1_u(f1)\n",
        "\n",
        "    f2 = torch.selu(self.input2_u(u[:,1*self.noi:2*self.noi].reshape(-1,self.noi)))\n",
        "    f2 = torch.selu(self.hidden2_u(f2))\n",
        "    f2 = self.output2_u(f2)\n",
        "\n",
        "    f3 = torch.selu(self.input3_u(u[:,2*self.noi:3*self.noi].reshape(-1,self.noi)))\n",
        "    f3 = torch.selu(self.hidden3_u(f3))\n",
        "    f3 = self.output3_u(f3)\n",
        "\n",
        "    f4 = torch.selu(self.input4_u(u[:,3*self.noi:4*self.noi].reshape(-1,self.noi)))\n",
        "    f4 = torch.selu(self.hidden4_u(f4))\n",
        "    f4 = self.output4_u(f4)\n",
        "\n",
        "    f = torch.cat((f1, f2, f3, f4), dim=1)\n",
        "\n",
        "    # sigma\n",
        "    s = sigma(t, self.k, self.n)\n",
        "\n",
        "    # g\n",
        "    g = torch.selu(self.input_t(t))\n",
        "    g = torch.selu(self.hidden_t(g))\n",
        "    g = self.output_t(g)\n",
        "\n",
        "    return torch.sum(h*f*s*g + x0, dim=1).reshape(-1,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BtNtb3epPIzJ"
      },
      "outputs": [],
      "source": [
        "# Model error\n",
        "def eval(model, testset):\n",
        "    with torch.no_grad():\n",
        "        pred_Y = model(testset.x0_data, testset.u_data, testset.t_data)\n",
        "\n",
        "    r2  = r2score(pred_Y, testset.y_data)\n",
        "    mse = msescore(pred_Y, testset.y_data)\n",
        "    return r2.item(), mse.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dTWtNEdVHCuw"
      },
      "outputs": [],
      "source": [
        "# Data\n",
        "class Data(torch.utils.data.Dataset):\n",
        "  def __init__(self, src_file, n, H, noi):\n",
        "    self.n   = n                                  # horizon length\n",
        "    self.H   = H                                  # max window length\n",
        "    self.noi = noi                                # number of input\n",
        "    self.src_file = src_file                      # source file\n",
        "    df = pd.read_csv(self.src_file, header=None)\n",
        "\n",
        "    X0, U, T, Y = np.array([[1]], dtype=np.float32), np.ones((1, n*self.noi)), np.array([[1]], dtype=np.float32), np.array([[1]], dtype=np.float32)\n",
        "    for i in range(df.shape[0]):\n",
        "        row = np.array(df.iloc[i])\n",
        "        for j in range(self.H - self.n):\n",
        "            x0 = np.array([[row[self.H*self.noi + j]]])\n",
        "            u  = np.array([row[j:j + self.n*self.noi]])\n",
        "            for t in range(1, self.n + 1):\n",
        "                y = np.array([[row[self.H*self.noi + j + t]]])\n",
        "                t = np.array([[t]])\n",
        "\n",
        "                X0 = np.concatenate((X0, x0))\n",
        "                U  = np.concatenate((U, u))\n",
        "                T  = np.concatenate((T, t))\n",
        "                Y  = np.concatenate((Y, y))\n",
        "\n",
        "    X0, U, T, Y = X0[1:], U[1:], T[1:], Y[1:]\n",
        "\n",
        "    self.x0_data = torch.tensor(X0, dtype=torch.float32)\n",
        "    self.u_data  = torch.tensor(U,  dtype=torch.float32)\n",
        "    self.t_data  = torch.tensor(T,  dtype=torch.float32)\n",
        "    self.y_data  = torch.tensor(Y,  dtype=torch.float32)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.x0_data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    if torch.is_tensor(idx):\n",
        "      idx = idx.tolist()\n",
        "    x0  = self.x0_data[idx]\n",
        "    u   = self.u_data[idx]\n",
        "    t   = self.t_data[idx]\n",
        "    y   = self.y_data[idx]\n",
        "    sample = {'x0':x0, 'u':u, 't':t, 'y':y}\n",
        "    return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Early stopping\n",
        "def early_stop(list, min_epochs, patience):\n",
        "    if(len(list) > min_epochs):\n",
        "        if(np.max(list[-patience:]) < 1.0001*np.max(list[0: -patience])):\n",
        "            return 1\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot\n",
        "def plot(net, dataset, size):\n",
        "    with torch.no_grad():\n",
        "        pred_Y = net(dataset.x0_data, dataset.u_data, dataset.t_data)\n",
        "\n",
        "    plt.figure(figsize=size)\n",
        "    plt.plot(dataset.y_data[0::4], 'b',   label=r'real',      linewidth=3)\n",
        "    plt.plot(pred_Y[0::4],         'r--', label=r'predicted', linewidth=1)\n",
        "    plt.ylabel(r'x(t)')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train function\n",
        "def train(net, train_ds, test_ds, lr=0.001, min_epochs=200, max_epochs=100000, patience=100):\n",
        "    loss_func  = torch.nn.MSELoss()\n",
        "    optimizer  = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "    train_ldr = torch.utils.data.DataLoader(train_ds, batch_size=train_ds.y_data.shape[0], shuffle=True)\n",
        "\n",
        "    R2  = np.array([])\n",
        "    MSE = np.array([])\n",
        "    for epoch in range(0, max_epochs+1):\n",
        "        net.train()\n",
        "        loss  = 0\n",
        "        count = 0\n",
        "        for (_, batch) in enumerate(train_ldr):\n",
        "            X0 = batch['x0']\n",
        "            U  = batch['u']\n",
        "            T  = batch['t']\n",
        "            Y  = batch['y']\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = net(X0, U, T)             # compute the output of the Network\n",
        "            loss_val = loss_func(output, Y)    # loss function\n",
        "            loss += loss_val.item()            # accumulate\n",
        "            loss_val.backward()                # gradients\n",
        "            optimizer.step()                   # update paramters\n",
        "            count += 1\n",
        "        \n",
        "        net.eval()\n",
        "        R2  = np.append(R2, eval(net, test_ds)[0])\n",
        "        MSE = np.append(MSE, eval(net, test_ds)[1])\n",
        "\n",
        "        # if(epoch%100==0):\n",
        "        #     print(\"epoch = %5d \\t loss = %12.4f \\t R2_train = %12.4f \\t R2_test = %12.4f\" % (epoch, loss/count, eval(net, train_ds), eval(net, test_ds)))\n",
        "        \n",
        "        if(early_stop(list = R2, min_epochs = min_epochs, patience = patience) == 1):\n",
        "            break\n",
        "    \n",
        "    return R2, MSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gebr4CnRLFBd",
        "outputId": "d74e7cd7-8dab-49a9-babc-e77577dd73b7"
      },
      "outputs": [],
      "source": [
        "df_result = pd.DataFrame({'lr':[], 'p':[], 'b0':[], 'bi':[], 'trunk':[], 'best_epoch':[], 'R2':[], 'MSE':[]})\n",
        "\n",
        "for _lr in [0.0001, 0.001, 0.01]:\n",
        "    for _p in [4, 8, 12]:\n",
        "        for b0 in [4, 8, 12]:\n",
        "            for bi in [2, 4, 8]:\n",
        "                for trunk in [4, 8, 12]:\n",
        "                    # Hyperparameters\n",
        "                    p   = _p          # size of branch and trunk ouput\n",
        "                    n   = 4           # horizon window length\n",
        "                    noi = 2           # number of inputs\n",
        "                    H   = 512         # maximum window length\n",
        "\n",
        "                    # Create Dataset and DataLoader objects\n",
        "                    src_file_train = '0. Data/data_0.csv'\n",
        "                    train_ds       = Data(src_file_train, n, H, noi)\n",
        "\n",
        "                    src_file_test  = '0. Data/data_1.csv'\n",
        "                    test_ds        = Data(src_file_test, n, H, noi)\n",
        "\n",
        "                    # Create network\n",
        "                    device = torch.device(\"cpu\")\n",
        "                    net = Net(n, p, noi, b0_size=b0, bi_size=bi, trunk_size=trunk).to(device)\n",
        "\n",
        "                    # train model\n",
        "                    lr         = _lr\n",
        "                    min_epochs = 1\n",
        "                    max_epochs = 2\n",
        "                    patience   = 1\n",
        "                    R2, MSE = train(net, train_ds, test_ds, lr, min_epochs, max_epochs, patience)\n",
        "\n",
        "                    # plot\n",
        "                    # plot(net, train_ds, (20,2))\n",
        "                    # plot(net, test_ds,  (20,2))\n",
        "\n",
        "                    # # best model\n",
        "                    # print(np.argmax(R2_test))\n",
        "                    # print(np.max(R2_test))\n",
        "\n",
        "                    df_result.loc[len(df_result)] = [_lr, _p, b0, bi, trunk, np.argmax(R2), np.max(R2), np.max(MSE)]\n",
        "\n",
        "                    # # save\n",
        "                    # PATH = '2. Saved model/DeepONet_HVAC.pt'\n",
        "                    # torch.save(net, PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_result.to_csv(\"stat.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         lr     p    b0   bi  trunk  best_epoch          R2         MSE\n",
            "0    0.0001   4.0   4.0  2.0    4.0         2.0 -1.6249e+04  5.1986e+04\n",
            "1    0.0001   4.0   4.0  2.0    8.0         2.0 -2.2862e+04  7.3131e+04\n",
            "2    0.0001   4.0   4.0  2.0   12.0         2.0 -1.4055e+04  4.4784e+04\n",
            "3    0.0001   4.0   4.0  4.0    4.0         2.0 -2.2251e+04  7.1650e+04\n",
            "4    0.0001   4.0   4.0  4.0    8.0         2.0 -1.6974e+04  5.4264e+04\n",
            "5    0.0001   4.0   4.0  4.0   12.0         2.0 -3.7079e+06  1.2115e+07\n",
            "6    0.0001   4.0   4.0  8.0    4.0         2.0 -6.7445e+05  2.1927e+06\n",
            "7    0.0001   4.0   4.0  8.0    8.0         2.0 -4.9131e+04  1.6010e+05\n",
            "8    0.0001   4.0   4.0  8.0   12.0         2.0 -1.8197e+05  6.0326e+05\n",
            "9    0.0001   4.0   8.0  2.0    4.0         2.0 -4.0783e+04  1.3221e+05\n",
            "10   0.0001   4.0   8.0  2.0    8.0         2.0 -2.6875e+04  8.6026e+04\n",
            "11   0.0001   4.0   8.0  2.0   12.0         2.0 -1.2234e+04  3.9203e+04\n",
            "12   0.0001   4.0   8.0  4.0    4.0         2.0 -2.6952e+04  8.7809e+04\n",
            "13   0.0001   4.0   8.0  4.0    8.0         2.0 -2.9214e+04  9.5384e+04\n",
            "14   0.0001   4.0   8.0  4.0   12.0         2.0 -3.7183e+04  1.2120e+05\n",
            "15   0.0001   4.0   8.0  8.0    4.0         2.0 -2.9372e+05  9.6534e+05\n",
            "16   0.0001   4.0   8.0  8.0    8.0         2.0 -4.2734e+04  1.3876e+05\n",
            "17   0.0001   4.0   8.0  8.0   12.0         2.0 -4.2244e+03  1.4398e+04\n",
            "18   0.0001   4.0  12.0  2.0    4.0         2.0 -1.3537e+04  4.3422e+04\n",
            "19   0.0001   4.0  12.0  2.0    8.0         2.0 -1.5313e+04  4.8925e+04\n",
            "20   0.0001   4.0  12.0  2.0   12.0         2.0 -1.4779e+04  4.7262e+04\n",
            "21   0.0001   4.0  12.0  4.0    4.0         2.0 -8.3081e+03  2.6778e+04\n",
            "22   0.0001   4.0  12.0  4.0    8.0         2.0 -1.5304e+04  4.8967e+04\n",
            "23   0.0001   4.0  12.0  4.0   12.0         2.0 -5.7473e+04  1.8896e+05\n",
            "24   0.0001   4.0  12.0  8.0    4.0         2.0 -5.7758e+04  1.9132e+05\n",
            "25   0.0001   4.0  12.0  8.0    8.0         2.0 -2.1381e+04  7.2450e+04\n",
            "26   0.0001   4.0  12.0  8.0   12.0         2.0 -2.9906e+04  9.8884e+04\n",
            "27   0.0001   8.0   4.0  2.0    4.0         2.0 -9.8441e+04  3.1439e+05\n",
            "28   0.0001   8.0   4.0  2.0    8.0         2.0 -7.4167e+04  2.3633e+05\n",
            "29   0.0001   8.0   4.0  2.0   12.0         2.0 -7.4707e+04  2.5107e+05\n",
            "30   0.0001   8.0   4.0  4.0    4.0         2.0 -1.6043e+05  5.2321e+05\n",
            "31   0.0001   8.0   4.0  4.0    8.0         2.0 -7.8809e+04  2.5463e+05\n",
            "32   0.0001   8.0   4.0  4.0   12.0         2.0 -8.5226e+04  2.7697e+05\n",
            "33   0.0001   8.0   4.0  8.0    4.0         2.0 -1.1336e+05  3.6333e+05\n",
            "34   0.0001   8.0   4.0  8.0    8.0         2.0 -1.1592e+05  3.7294e+05\n",
            "35   0.0001   8.0   4.0  8.0   12.0         2.0 -2.6410e+04  8.7649e+04\n",
            "36   0.0001   8.0   8.0  2.0    4.0         2.0 -2.2881e+04  7.6096e+04\n",
            "37   0.0001   8.0   8.0  2.0    8.0         2.0 -4.3150e+04  1.3887e+05\n",
            "38   0.0001   8.0   8.0  2.0   12.0         2.0 -1.1821e+05  3.9115e+05\n",
            "39   0.0001   8.0   8.0  4.0    4.0         2.0 -8.3382e+04  2.6642e+05\n",
            "40   0.0001   8.0   8.0  4.0    8.0         2.0 -6.8369e+04  2.2058e+05\n",
            "41   0.0001   8.0   8.0  4.0   12.0         2.0 -7.6543e+04  2.4627e+05\n",
            "42   0.0001   8.0   8.0  8.0    4.0         2.0 -7.8697e+04  2.5120e+05\n",
            "43   0.0001   8.0   8.0  8.0    8.0         2.0 -1.3423e+05  4.3067e+05\n",
            "44   0.0001   8.0   8.0  8.0   12.0         2.0 -6.6781e+04  2.1630e+05\n",
            "45   0.0001   8.0  12.0  2.0    4.0         2.0 -6.1106e+04  1.9544e+05\n",
            "46   0.0001   8.0  12.0  2.0    8.0         2.0 -8.6403e+04  2.7823e+05\n",
            "47   0.0001   8.0  12.0  2.0   12.0         2.0 -1.7861e+05  6.2800e+05\n",
            "48   0.0001   8.0  12.0  4.0    4.0         2.0 -1.6034e+05  5.1564e+05\n",
            "49   0.0001   8.0  12.0  4.0    8.0         2.0 -6.5936e+04  2.1296e+05\n",
            "50   0.0001   8.0  12.0  4.0   12.0         2.0 -1.0455e+05  3.5134e+05\n",
            "51   0.0001   8.0  12.0  8.0    4.0         2.0 -2.1418e+05  7.0945e+05\n",
            "52   0.0001   8.0  12.0  8.0    8.0         2.0 -2.8842e+04  9.9472e+04\n",
            "53   0.0001   8.0  12.0  8.0   12.0         2.0 -7.8299e+04  2.5874e+05\n",
            "54   0.0001  12.0   4.0  2.0    4.0         2.0 -7.1561e+04  2.2949e+05\n",
            "55   0.0001  12.0   4.0  2.0    8.0         2.0 -1.0846e+05  3.5589e+05\n",
            "56   0.0001  12.0   4.0  2.0   12.0         2.0 -3.1854e+04  1.0699e+05\n",
            "57   0.0001  12.0   4.0  4.0    4.0         2.0 -1.6799e+05  5.3744e+05\n",
            "58   0.0001  12.0   4.0  4.0    8.0         2.0 -1.7060e+05  5.4688e+05\n",
            "59   0.0001  12.0   4.0  4.0   12.0         2.0 -2.4704e+05  8.1129e+05\n",
            "60   0.0001  12.0   4.0  8.0    4.0         2.0 -1.8937e+05  6.0354e+05\n",
            "61   0.0001  12.0   4.0  8.0    8.0         2.0 -2.8974e+05  9.3684e+05\n",
            "62   0.0001  12.0   4.0  8.0   12.0         2.0 -3.1333e+05  1.0170e+06\n",
            "63   0.0001  12.0   8.0  2.0    4.0         2.0 -2.1047e+05  6.7199e+05\n",
            "64   0.0001  12.0   8.0  2.0    8.0         2.0 -1.2474e+05  4.0009e+05\n",
            "65   0.0001  12.0   8.0  2.0   12.0         2.0 -2.5406e+05  8.1454e+05\n",
            "66   0.0001  12.0   8.0  4.0    4.0         2.0 -3.3320e+05  1.0792e+06\n",
            "67   0.0001  12.0   8.0  4.0    8.0         2.0 -7.4122e+04  2.4223e+05\n",
            "68   0.0001  12.0   8.0  4.0   12.0         2.0 -2.2219e+05  7.1401e+05\n",
            "69   0.0001  12.0   8.0  8.0    4.0         2.0 -1.8712e+05  5.9720e+05\n",
            "70   0.0001  12.0   8.0  8.0    8.0         2.0 -1.1973e+05  3.9661e+05\n",
            "71   0.0001  12.0   8.0  8.0   12.0         2.0 -1.6272e+06  5.3355e+06\n",
            "72   0.0001  12.0  12.0  2.0    4.0         2.0 -3.8172e+04  1.2695e+05\n",
            "73   0.0001  12.0  12.0  2.0    8.0         2.0 -3.4572e+05  1.1280e+06\n",
            "74   0.0001  12.0  12.0  2.0   12.0         2.0 -7.2450e+04  2.4369e+05\n",
            "75   0.0001  12.0  12.0  4.0    4.0         2.0 -6.8962e+04  2.2389e+05\n",
            "76   0.0001  12.0  12.0  4.0    8.0         2.0 -2.1619e+05  6.9194e+05\n",
            "77   0.0001  12.0  12.0  4.0   12.0         2.0 -1.2631e+05  4.1049e+05\n",
            "78   0.0001  12.0  12.0  8.0    4.0         2.0 -4.8398e+04  1.6170e+05\n",
            "79   0.0001  12.0  12.0  8.0    8.0         2.0 -1.2743e+05  4.1406e+05\n",
            "80   0.0001  12.0  12.0  8.0   12.0         2.0 -2.6150e+05  8.8397e+05\n",
            "81   0.0010   4.0   4.0  2.0    4.0         2.0 -2.7764e+03  9.0273e+03\n",
            "82   0.0010   4.0   4.0  2.0    8.0         2.0 -1.0892e+04  3.5506e+04\n",
            "83   0.0010   4.0   4.0  2.0   12.0         2.0 -1.4311e+04  4.5946e+04\n",
            "84   0.0010   4.0   4.0  4.0    4.0         2.0 -5.4970e+03  1.9291e+04\n",
            "85   0.0010   4.0   4.0  4.0    8.0         2.0 -2.6215e+03  1.1410e+04\n",
            "86   0.0010   4.0   4.0  4.0   12.0         2.0 -7.8562e+03  2.8062e+04\n",
            "87   0.0010   4.0   4.0  8.0    4.0         2.0 -2.5631e+04  9.0539e+04\n",
            "88   0.0010   4.0   4.0  8.0    8.0         2.0 -3.2804e+04  1.3338e+05\n",
            "89   0.0010   4.0   4.0  8.0   12.0         2.0 -1.1211e+05  4.9450e+05\n",
            "90   0.0010   4.0   8.0  2.0    4.0         2.0 -4.5702e+03  1.7672e+04\n",
            "91   0.0010   4.0   8.0  2.0    8.0         2.0 -4.4241e+04  1.7454e+05\n",
            "92   0.0010   4.0   8.0  2.0   12.0         2.0 -6.4483e+04  2.6667e+05\n",
            "93   0.0010   4.0   8.0  4.0    4.0         2.0 -2.7777e+03  1.0799e+04\n",
            "94   0.0010   4.0   8.0  4.0    8.0         2.0 -1.0453e+05  4.0862e+05\n",
            "95   0.0010   4.0   8.0  4.0   12.0         2.0 -1.4059e+04  4.7632e+04\n",
            "96   0.0010   4.0   8.0  8.0    4.0         2.0 -1.5224e+04  5.2029e+04\n",
            "97   0.0010   4.0   8.0  8.0    8.0         2.0 -3.3648e+04  1.3995e+05\n",
            "98   0.0010   4.0   8.0  8.0   12.0         2.0 -1.7848e+03  1.6739e+04\n",
            "99   0.0010   4.0  12.0  2.0    4.0         2.0 -1.7492e+04  5.7153e+04\n",
            "100  0.0010   4.0  12.0  2.0    8.0         2.0 -1.1915e+04  3.9989e+04\n",
            "101  0.0010   4.0  12.0  2.0   12.0         2.0 -1.6673e+04  5.7699e+04\n",
            "102  0.0010   4.0  12.0  4.0    4.0         2.0 -4.0869e+04  1.5957e+05\n",
            "103  0.0010   4.0  12.0  4.0    8.0         2.0 -9.2416e+03  3.2594e+04\n",
            "104  0.0010   4.0  12.0  4.0   12.0         2.0 -1.2408e+04  4.2823e+04\n",
            "105  0.0010   4.0  12.0  8.0    4.0         2.0 -7.4654e+03  2.7629e+04\n",
            "106  0.0010   4.0  12.0  8.0    8.0         2.0 -2.7561e+04  1.2049e+05\n",
            "107  0.0010   4.0  12.0  8.0   12.0         2.0 -4.0837e+04  1.8392e+05\n",
            "108  0.0010   8.0   4.0  2.0    4.0         2.0 -5.1657e+04  1.6828e+05\n",
            "109  0.0010   8.0   4.0  2.0    8.0         2.0 -1.2045e+05  4.1174e+05\n",
            "110  0.0010   8.0   4.0  2.0   12.0         2.0 -9.1767e+04  3.0368e+05\n",
            "111  0.0010   8.0   4.0  4.0    4.0         2.0 -7.7542e+04  2.4783e+05\n",
            "112  0.0010   8.0   4.0  4.0    8.0         2.0 -1.1343e+05  4.1862e+05\n",
            "113  0.0010   8.0   4.0  4.0   12.0         0.0 -1.5254e+04  4.8649e+04\n",
            "114  0.0010   8.0   4.0  8.0    4.0         2.0 -1.0827e+05  3.8721e+05\n",
            "115  0.0010   8.0   4.0  8.0    8.0         2.0 -1.3098e+05  4.8252e+05\n",
            "116  0.0010   8.0   4.0  8.0   12.0         2.0 -6.4026e+04  2.2284e+05\n",
            "117  0.0010   8.0   8.0  2.0    4.0         2.0 -7.6820e+04  2.4560e+05\n",
            "118  0.0010   8.0   8.0  2.0    8.0         2.0 -1.0544e+05  3.6262e+05\n",
            "119  0.0010   8.0   8.0  2.0   12.0         2.0 -1.2969e+05  4.8089e+05\n",
            "120  0.0010   8.0   8.0  4.0    4.0         2.0 -7.2242e+04  2.5881e+05\n",
            "121  0.0010   8.0   8.0  4.0    8.0         2.0 -1.2274e+05  4.5695e+05\n",
            "122  0.0010   8.0   8.0  4.0   12.0         2.0 -7.0238e+04  2.3688e+05\n",
            "123  0.0010   8.0   8.0  8.0    4.0         2.0 -6.2422e+04  2.7667e+05\n",
            "124  0.0010   8.0   8.0  8.0    8.0         2.0 -1.5661e+05  6.0274e+05\n",
            "125  0.0010   8.0   8.0  8.0   12.0         2.0 -5.4886e+04  2.0921e+05\n",
            "126  0.0010   8.0  12.0  2.0    4.0         2.0 -6.8404e+04  2.2545e+05\n",
            "127  0.0010   8.0  12.0  2.0    8.0         2.0 -5.8898e+04  2.0622e+05\n",
            "128  0.0010   8.0  12.0  2.0   12.0         2.0 -5.6723e+04  2.0454e+05\n",
            "129  0.0010   8.0  12.0  4.0    4.0         2.0 -7.6824e+04  3.4404e+05\n",
            "130  0.0010   8.0  12.0  4.0    8.0         2.0 -5.7033e+04  2.5563e+05\n",
            "131  0.0010   8.0  12.0  4.0   12.0         2.0 -1.0638e+05  5.4282e+05\n",
            "132  0.0010   8.0  12.0  8.0    4.0         2.0 -2.8757e+05  1.1758e+06\n",
            "133  0.0010   8.0  12.0  8.0    8.0         2.0 -4.1691e+05  1.7198e+06\n",
            "134  0.0010   8.0  12.0  8.0   12.0         2.0 -5.0822e+04  2.0391e+05\n",
            "135  0.0010  12.0   4.0  2.0    4.0         2.0 -1.5854e+05  5.1633e+05\n",
            "136  0.0010  12.0   4.0  2.0    8.0         2.0 -1.1676e+05  4.3654e+05\n",
            "137  0.0010  12.0   4.0  2.0   12.0         2.0 -1.8801e+05  6.0644e+05\n",
            "138  0.0010  12.0   4.0  4.0    4.0         2.0 -2.1810e+05  7.2750e+05\n",
            "139  0.0010  12.0   4.0  4.0    8.0         2.0 -2.3328e+05  7.6773e+05\n",
            "140  0.0010  12.0   4.0  4.0   12.0         2.0 -1.3975e+05  4.8154e+05\n",
            "141  0.0010  12.0   4.0  8.0    4.0         2.0 -1.5856e+05  5.5392e+05\n",
            "142  0.0010  12.0   4.0  8.0    8.0         2.0 -1.7286e+05  6.8520e+05\n",
            "143  0.0010  12.0   4.0  8.0   12.0         2.0 -6.2092e+04  2.9841e+05\n",
            "144  0.0010  12.0   8.0  2.0    4.0         2.0 -2.6893e+05  1.1035e+06\n",
            "145  0.0010  12.0   8.0  2.0    8.0         2.0 -1.7245e+05  5.7462e+05\n",
            "146  0.0010  12.0   8.0  2.0   12.0         2.0 -2.2830e+05  8.5356e+05\n",
            "147  0.0010  12.0   8.0  4.0    4.0         2.0 -2.7067e+05  9.4458e+05\n",
            "148  0.0010  12.0   8.0  4.0    8.0         2.0 -3.9022e+05  1.4860e+06\n",
            "149  0.0010  12.0   8.0  4.0   12.0         2.0 -5.2419e+04  2.2135e+05\n",
            "150  0.0010  12.0   8.0  8.0    4.0         2.0 -1.2054e+05  6.2834e+05\n",
            "151  0.0010  12.0   8.0  8.0    8.0         2.0 -7.8318e+04  3.4935e+05\n",
            "152  0.0010  12.0   8.0  8.0   12.0         2.0 -3.2661e+05  1.4701e+06\n",
            "153  0.0010  12.0  12.0  2.0    4.0         2.0 -9.7640e+04  3.6432e+05\n",
            "154  0.0010  12.0  12.0  2.0    8.0         2.0 -2.0147e+05  7.1153e+05\n",
            "155  0.0010  12.0  12.0  2.0   12.0         2.0 -2.5478e+04  1.3194e+05\n",
            "156  0.0010  12.0  12.0  4.0    4.0         2.0 -1.6171e+05  5.4551e+05\n",
            "157  0.0010  12.0  12.0  4.0    8.0         2.0 -8.8104e+04  3.7317e+05\n",
            "158  0.0010  12.0  12.0  4.0   12.0         2.0 -1.2565e+05  5.5961e+05\n",
            "159  0.0010  12.0  12.0  8.0    4.0         2.0 -4.0362e+05  1.5532e+06\n",
            "160  0.0010  12.0  12.0  8.0    8.0         2.0 -1.8488e+05  7.8766e+05\n",
            "161  0.0010  12.0  12.0  8.0   12.0         2.0 -1.3338e+05  5.8246e+05\n",
            "162  0.0100   4.0   4.0  2.0    4.0         2.0 -1.4230e+04  4.6230e+04\n",
            "163  0.0100   4.0   4.0  2.0    8.0         2.0 -4.5477e+03  3.9451e+04\n",
            "164  0.0100   4.0   4.0  2.0   12.0         2.0 -8.9506e+03  9.1274e+04\n",
            "165  0.0100   4.0   4.0  4.0    4.0         2.0 -5.6857e+03  7.6756e+04\n",
            "166  0.0100   4.0   4.0  4.0    8.0         2.0 -1.1646e+04  4.4106e+04\n",
            "167  0.0100   4.0   4.0  4.0   12.0         1.0 -1.4765e+03  8.0317e+03\n",
            "168  0.0100   4.0   4.0  8.0    4.0         2.0 -1.0870e+04  5.1028e+04\n",
            "169  0.0100   4.0   4.0  8.0    8.0         2.0 -1.2574e+04  4.5244e+04\n",
            "170  0.0100   4.0   4.0  8.0   12.0         2.0 -3.6184e+03  2.4889e+04\n",
            "171  0.0100   4.0   8.0  2.0    4.0         2.0 -5.4234e+03  3.1364e+04\n",
            "172  0.0100   4.0   8.0  2.0    8.0         2.0 -8.6243e+03  3.3312e+04\n",
            "173  0.0100   4.0   8.0  2.0   12.0         2.0 -4.5346e+03  1.9823e+04\n",
            "174  0.0100   4.0   8.0  4.0    4.0         0.0 -7.9152e+03  4.3629e+04\n",
            "175  0.0100   4.0   8.0  4.0    8.0         2.0 -8.4330e+03  3.8984e+04\n",
            "176  0.0100   4.0   8.0  4.0   12.0         0.0 -5.7321e+03  3.0077e+04\n",
            "177  0.0100   4.0   8.0  8.0    4.0         1.0 -4.9485e+03  2.1083e+04\n",
            "178  0.0100   4.0   8.0  8.0    8.0         2.0 -9.5748e+03  4.8950e+04\n",
            "179  0.0100   4.0   8.0  8.0   12.0         1.0 -5.5031e+03  5.6373e+04\n",
            "180  0.0100   4.0  12.0  2.0    4.0         2.0 -8.8134e+03  3.9172e+04\n",
            "181  0.0100   4.0  12.0  2.0    8.0         2.0 -7.8091e+03  5.8756e+05\n",
            "182  0.0100   4.0  12.0  2.0   12.0         2.0 -8.7753e+03  1.1964e+05\n",
            "183  0.0100   4.0  12.0  4.0    4.0         2.0 -7.4838e+03  2.9380e+04\n",
            "184  0.0100   4.0  12.0  4.0    8.0         2.0 -2.8803e+03  2.2916e+04\n",
            "185  0.0100   4.0  12.0  4.0   12.0         2.0 -3.7839e+03  1.0604e+05\n",
            "186  0.0100   4.0  12.0  8.0    4.0         2.0 -5.1709e+03  3.4361e+04\n",
            "187  0.0100   4.0  12.0  8.0    8.0         2.0 -4.1817e+03  3.0233e+04\n",
            "188  0.0100   4.0  12.0  8.0   12.0         2.0 -4.0530e+03  2.1121e+04\n",
            "189  0.0100   8.0   4.0  2.0    4.0         2.0 -4.3163e+04  1.9390e+05\n",
            "190  0.0100   8.0   4.0  2.0    8.0         2.0 -5.9028e+04  2.3298e+05\n",
            "191  0.0100   8.0   4.0  2.0   12.0         2.0 -3.8992e+04  2.2581e+05\n",
            "192  0.0100   8.0   4.0  4.0    4.0         2.0 -4.2636e+04  2.3520e+05\n",
            "193  0.0100   8.0   4.0  4.0    8.0         2.0 -9.7742e+03  1.5277e+05\n",
            "194  0.0100   8.0   4.0  4.0   12.0         2.0 -1.8756e+04  1.6909e+05\n",
            "195  0.0100   8.0   4.0  8.0    4.0         2.0 -6.8840e+04  9.7176e+05\n",
            "196  0.0100   8.0   4.0  8.0    8.0         1.0 -1.9482e+04  2.1940e+05\n",
            "197  0.0100   8.0   4.0  8.0   12.0         2.0 -2.0518e+04  3.0520e+05\n",
            "198  0.0100   8.0   8.0  2.0    4.0         1.0 -2.5559e+04  1.6386e+05\n",
            "199  0.0100   8.0   8.0  2.0    8.0         2.0 -3.1968e+04  1.5306e+05\n",
            "200  0.0100   8.0   8.0  2.0   12.0         1.0 -1.5243e+04  8.6844e+04\n",
            "201  0.0100   8.0   8.0  4.0    4.0         2.0 -2.2825e+04  1.4843e+05\n",
            "202  0.0100   8.0   8.0  4.0    8.0         2.0 -4.3700e+04  2.2559e+05\n",
            "203  0.0100   8.0   8.0  4.0   12.0         2.0 -1.2621e+05  2.3399e+06\n",
            "204  0.0100   8.0   8.0  8.0    4.0         2.0 -6.4393e+03  1.9202e+05\n",
            "205  0.0100   8.0   8.0  8.0    8.0         2.0 -3.6143e+04  2.4657e+05\n",
            "206  0.0100   8.0   8.0  8.0   12.0         1.0 -1.3161e+04  5.3853e+04\n",
            "207  0.0100   8.0  12.0  2.0    4.0         2.0 -6.8487e+04  2.7627e+05\n",
            "208  0.0100   8.0  12.0  2.0    8.0         2.0 -6.7211e+04  2.8937e+05\n",
            "209  0.0100   8.0  12.0  2.0   12.0         1.0 -4.9395e+04  2.0069e+05\n",
            "210  0.0100   8.0  12.0  4.0    4.0         2.0 -2.5148e+04  1.9429e+05\n",
            "211  0.0100   8.0  12.0  4.0    8.0         2.0 -5.9142e+04  6.1794e+05\n",
            "212  0.0100   8.0  12.0  4.0   12.0         1.0 -2.6804e+04  1.0222e+05\n",
            "213  0.0100   8.0  12.0  8.0    4.0         2.0 -1.9795e+04  1.4194e+05\n",
            "214  0.0100   8.0  12.0  8.0    8.0         1.0 -1.1527e+04  9.3643e+04\n",
            "215  0.0100   8.0  12.0  8.0   12.0         2.0 -1.6178e+04  8.9996e+04\n",
            "216  0.0100  12.0   4.0  2.0    4.0         2.0 -3.1031e+05  1.8593e+06\n",
            "217  0.0100  12.0   4.0  2.0    8.0         2.0 -1.0363e+05  7.7238e+05\n",
            "218  0.0100  12.0   4.0  2.0   12.0         2.0 -7.9794e+04  3.7762e+05\n",
            "219  0.0100  12.0   4.0  4.0    4.0         2.0 -1.5727e+05  5.9430e+05\n",
            "220  0.0100  12.0   4.0  4.0    8.0         2.0 -1.4955e+05  5.8833e+05\n",
            "221  0.0100  12.0   4.0  4.0   12.0         2.0 -4.0613e+04  1.7222e+05\n",
            "222  0.0100  12.0   4.0  8.0    4.0         2.0 -1.0016e+05  1.0688e+06\n",
            "223  0.0100  12.0   4.0  8.0    8.0         2.0 -4.9518e+04  1.6522e+06\n",
            "224  0.0100  12.0   4.0  8.0   12.0         2.0 -2.0328e+04  3.2161e+05\n",
            "225  0.0100  12.0   8.0  2.0    4.0         2.0 -1.3498e+05  5.5757e+05\n",
            "226  0.0100  12.0   8.0  2.0    8.0         2.0 -1.4059e+05  5.6946e+05\n",
            "227  0.0100  12.0   8.0  2.0   12.0         2.0 -1.1774e+05  6.0803e+05\n",
            "228  0.0100  12.0   8.0  4.0    4.0         2.0 -1.0331e+05  6.7994e+05\n",
            "229  0.0100  12.0   8.0  4.0    8.0         2.0 -4.8696e+04  4.8165e+05\n",
            "230  0.0100  12.0   8.0  4.0   12.0         2.0 -5.0917e+04  1.9688e+05\n",
            "231  0.0100  12.0   8.0  8.0    4.0         2.0 -8.1409e+04  5.8199e+05\n",
            "232  0.0100  12.0   8.0  8.0    8.0         2.0 -3.1164e+04  4.3920e+05\n",
            "233  0.0100  12.0   8.0  8.0   12.0         1.0 -5.7032e+04  3.8207e+05\n",
            "234  0.0100  12.0  12.0  2.0    4.0         2.0 -4.0639e+04  5.8017e+05\n",
            "235  0.0100  12.0  12.0  2.0    8.0         1.0 -1.1725e+05  4.1108e+05\n",
            "236  0.0100  12.0  12.0  2.0   12.0         2.0 -2.0773e+04  9.6027e+04\n",
            "237  0.0100  12.0  12.0  4.0    4.0         2.0 -1.9978e+05  1.7467e+06\n",
            "238  0.0100  12.0  12.0  4.0    8.0         2.0 -1.0222e+05  4.9802e+05\n",
            "239  0.0100  12.0  12.0  4.0   12.0         2.0 -7.4959e+04  4.6918e+05\n",
            "240  0.0100  12.0  12.0  8.0    4.0         2.0 -8.1466e+04  3.1261e+05\n",
            "241  0.0100  12.0  12.0  8.0    8.0         2.0 -4.0209e+04  3.4929e+05\n",
            "242  0.0100  12.0  12.0  8.0   12.0         2.0 -5.9009e+04  4.3699e+05\n"
          ]
        }
      ],
      "source": [
        "with pd.option_context('display.max_rows', None,\n",
        "                       'display.max_columns', None,\n",
        "                       'display.precision', 4,\n",
        "                       ):\n",
        "    print(df_result)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "3. T_z .ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "912d6611990680b3d240e982c9d50f3da4c776707cfd42695cf7d82c88d80956"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
